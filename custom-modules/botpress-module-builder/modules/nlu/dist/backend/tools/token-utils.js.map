{"version":3,"sources":["tools/token-utils.ts"],"names":["SPACE","isWord","str","_","every","SPECIAL_CHARSET","c","RegExp","test","hasSpace","some","isSpace","convertToRealSpaces","replace","splitSpaceToken","token","split","filter","identity","tokenizeLatinTextForTests","text","mergeSimilarCharsetTokens","tokens","charPatterns","matcher","charMatcher","join","reduce","mergedToks","nextTok","prev","last","slice","length","mergeSpaces","mergeNumeral","mergeSpecialChars","mergeLatin","vocab","oovMatcher","toLowerCase","LATIN_CHARSET","processUtteranceTokens","chain","flatMap","thru","startsWith","value","restoreOriginalUtteranceCasing","utteranceTokens","utterance","offset","map","t","original","substr"],"mappings":";;;;;;;;AAAA;;AAIA;;;;AAEO,MAAMA,KAAK,GAAG,QAAd;;;AAEA,MAAMC,MAAM,GAAIC,GAAD,IAAiBC,gBAAEC,KAAF,CAAQC,sBAAR,EAAyBC,CAAC,IAAI,CAACC,MAAM,CAACD,CAAD,CAAN,CAAUE,IAAV,CAAeN,GAAf,CAA/B,KAAuD,CAACO,QAAQ,CAACP,GAAD,CAAhG;;;;AAEA,MAAMO,QAAQ,GAAIP,GAAD,IAAiBC,gBAAEO,IAAF,CAAOR,GAAP,EAAYS,OAAZ,CAAlC;;;;AAEA,MAAMA,OAAO,GAAIT,GAAD,IAAiBC,gBAAEC,KAAF,CAAQF,GAAR,EAAaI,CAAC,IAAIA,CAAC,KAAKN,KAAN,IAAeM,CAAC,KAAK,GAAvC,CAAjC;;;;AAEA,MAAMM,mBAAmB,GAAIV,GAAD,IAAiBA,GAAG,CAACW,OAAJ,CAAY,IAAIN,MAAJ,CAAWP,KAAX,EAAkB,GAAlB,CAAZ,EAAoC,GAApC,CAA7C;;;;AAEP,SAASc,eAAT,CAAyBC,KAAzB,EAAkD;AAChD,SAAOA,KAAK,CAACC,KAAN,CAAY,IAAIT,MAAJ,CAAY,IAAGP,KAAM,GAArB,EAAyB,GAAzB,CAAZ,EAA2CiB,MAA3C,CAAkDd,gBAAEe,QAApD,CAAP;AACD;AAED;;;;;;AAIO,SAASC,yBAAT,CAAmCC,IAAnC,EAA2D;AAChE,SAAON,eAAe,CAACM,IAAI,CAACP,OAAL,CAAa,KAAb,EAAoBb,KAApB,CAAD,CAAtB;AACD;;AAID;;;;;;;;AAQO,MAAMqB,yBAAyB,GAAG,CACvCC,MADuC,EAEvCC,YAFuC,EAGvCC,OAAsB,GAAG,MAAM,IAHQ,KAI1B;AACb,QAAMC,WAAW,GAAG,IAAIlB,MAAJ,CAAY,KAAIgB,YAAY,CAACG,IAAb,CAAkB,GAAlB,CAAuB,KAAvC,EAA6C,GAA7C,CAApB;AACA,SAAOJ,MAAM,CAACK,MAAP,CAAc,CAACC,UAAD,EAAuBC,OAAvB,KAA2C;AAC9D,UAAMC,IAAI,GAAG3B,gBAAE4B,IAAF,CAAOH,UAAP,CAAb;;AACA,QAAIE,IAAI,IAAIL,WAAW,CAACjB,IAAZ,CAAiBsB,IAAjB,CAAR,IAAkCL,WAAW,CAACjB,IAAZ,CAAiBqB,OAAjB,CAAlC,KAAgEL,OAAO,CAACM,IAAD,CAAP,IAAiBN,OAAO,CAACK,OAAD,CAAxF,CAAJ,EAAwG;AACtG,aAAO,CAAC,GAAGD,UAAU,CAACI,KAAX,CAAiB,CAAjB,EAAoBJ,UAAU,CAACK,MAAX,GAAoB,CAAxC,CAAJ,EAAiD,GAAE9B,gBAAE4B,IAAF,CAAOH,UAAP,KAAsB,EAAG,GAAEC,OAAQ,EAAtF,CAAP;AACD,KAFD,MAEO;AACL,aAAO,CAAC,GAAGD,UAAJ,EAAgBC,OAAhB,CAAP;AACD;AACF,GAPM,EAOJ,EAPI,CAAP;AAQD,CAdM;;;;AAgBP,MAAMK,WAAW,GAAIZ,MAAD,IAAgCD,yBAAyB,CAACC,MAAD,EAAS,CAACtB,KAAD,CAAT,CAA7E;;AACA,MAAMmC,YAAY,GAAIb,MAAD,IAAgCD,yBAAyB,CAACC,MAAD,EAAS,CAAC,OAAD,CAAT,CAA9E;;AACA,MAAMc,iBAAiB,GAAId,MAAD,IAAgCD,yBAAyB,CAACC,MAAD,EAASjB,sBAAT,CAAnF;;AACA,MAAMgC,UAAU,GAAG,CAACf,MAAD,EAAmBgB,KAAnB,KAAkD;AACnE,QAAMC,UAAU,GAAIxB,KAAD,IAAmB;AACpC,WAAOA,KAAK,IAAI,CAACuB,KAAK,CAACvB,KAAK,CAACyB,WAAN,EAAD,CAAtB;AACD,GAFD;;AAGA,SAAOnB,yBAAyB,CAACC,MAAD,EAASmB,oBAAT,EAAwBF,UAAxB,CAAhC;AACD,CALD;;AAOO,MAAMG,sBAAsB,GAAG,CAACpB,MAAD,EAAmBgB,KAAgB,GAAG,EAAtC,KAAuD;AAC3F,SAAOnC,gBAAEwC,KAAF,CAAQrB,MAAR,EACJsB,OADI,CACI9B,eADJ,EAEJ+B,IAFI,CAECX,WAFD,EAGJW,IAHI,CAGCV,YAHD,EAIJU,IAJI,CAICT,iBAJD,EAKJS,IALI,CAKCvB,MAAM,IAAIe,UAAU,CAACf,MAAD,EAASgB,KAAT,CALrB,EAMJO,IANI,CAMCvB,MAAM,IAAKA,MAAM,CAACW,MAAP,IAAiBX,MAAM,CAAC,CAAD,CAAN,CAAUwB,UAAV,CAAqB9C,KAArB,CAAjB,GAA+CsB,MAAM,CAACU,KAAP,CAAa,CAAb,CAA/C,GAAiEV,MAN7E,EAMsF;AANtF,GAOJyB,KAPI,EAAP;AAQD,CATM;;;;AAWA,MAAMC,8BAA8B,GAAG,CAACC,eAAD,EAA4BC,SAA5B,KAA4D;AACxG,MAAIC,MAAM,GAAG,CAAb;AACA,SAAOF,eAAe,CAACG,GAAhB,CAAoBC,CAAC,IAAI;AAC9B,UAAMC,QAAQ,GAAG3C,OAAO,CAAC0C,CAAD,CAAP,GAAaA,CAAb,GAAiBH,SAAS,CAACK,MAAV,CAAiBJ,MAAjB,EAAyBE,CAAC,CAACpB,MAA3B,CAAlC;AACAkB,IAAAA,MAAM,IAAIE,CAAC,CAACpB,MAAZ;AACA,WAAOqB,QAAP;AACD,GAJM,CAAP;AAKD,CAPM","sourceRoot":"/src/modules/nlu/src/backend","sourcesContent":["import _ from 'lodash'\n\nimport { Token2Vec } from '../typings'\n\nimport { LATIN_CHARSET, SPECIAL_CHARSET } from './chars'\n\nexport const SPACE = '\\u2581'\n\nexport const isWord = (str: string) => _.every(SPECIAL_CHARSET, c => !RegExp(c).test(str)) && !hasSpace(str)\n\nexport const hasSpace = (str: string) => _.some(str, isSpace)\n\nexport const isSpace = (str: string) => _.every(str, c => c === SPACE || c === ' ')\n\nexport const convertToRealSpaces = (str: string) => str.replace(new RegExp(SPACE, 'g'), ' ')\n\nfunction splitSpaceToken(token: string): string[] {\n  return token.split(new RegExp(`(${SPACE})`, 'g')).filter(_.identity)\n}\n\n/**\n * Basically mimics the language server tokenizer. Use this function for testing purposes\n * @param text text you want to tokenize\n */\nexport function tokenizeLatinTextForTests(text: string): string[] {\n  return splitSpaceToken(text.replace(/\\s/g, SPACE))\n}\n\ntype CustomMatcher = (tok: string) => boolean\n\n/**\n * Merges consecutive tokens that all respect the provided regex\n * @param tokens list of string representing a sentence\n * @param charPatterns (string patterns) that **every** characters in a token **can** match\n * @param matcher custom matcher function called on each token\n * @example ['13', 'lo', '34', '56'] with a char pool of numbers ==> ['13', 'lo', '3456']\n * @example ['_', '__', '_', 'abc'] with a char pool of ['_'] ==> ['____', 'abc']\n */\nexport const mergeSimilarCharsetTokens = (\n  tokens: string[],\n  charPatterns: string[],\n  matcher: CustomMatcher = () => true\n): string[] => {\n  const charMatcher = new RegExp(`^(${charPatterns.join('|')})+$`, 'i')\n  return tokens.reduce((mergedToks: string[], nextTok: string) => {\n    const prev = _.last(mergedToks)\n    if (prev && charMatcher.test(prev) && charMatcher.test(nextTok) && (matcher(prev) || matcher(nextTok))) {\n      return [...mergedToks.slice(0, mergedToks.length - 1), `${_.last(mergedToks) || ''}${nextTok}`]\n    } else {\n      return [...mergedToks, nextTok]\n    }\n  }, [])\n}\n\nconst mergeSpaces = (tokens: string[]): string[] => mergeSimilarCharsetTokens(tokens, [SPACE])\nconst mergeNumeral = (tokens: string[]): string[] => mergeSimilarCharsetTokens(tokens, ['[0-9]'])\nconst mergeSpecialChars = (tokens: string[]): string[] => mergeSimilarCharsetTokens(tokens, SPECIAL_CHARSET)\nconst mergeLatin = (tokens: string[], vocab: Token2Vec): string[] => {\n  const oovMatcher = (token: string) => {\n    return token && !vocab[token.toLowerCase()]\n  }\n  return mergeSimilarCharsetTokens(tokens, LATIN_CHARSET, oovMatcher)\n}\n\nexport const processUtteranceTokens = (tokens: string[], vocab: Token2Vec = {}): string[] => {\n  return _.chain(tokens)\n    .flatMap(splitSpaceToken)\n    .thru(mergeSpaces)\n    .thru(mergeNumeral)\n    .thru(mergeSpecialChars)\n    .thru(tokens => mergeLatin(tokens, vocab))\n    .thru(tokens => (tokens.length && tokens[0].startsWith(SPACE) ? tokens.slice(1) : tokens)) // remove 1st token if space, even if input trimmed, sometimes tokenizer returns space char\n    .value()\n}\n\nexport const restoreOriginalUtteranceCasing = (utteranceTokens: string[], utterance: string): string[] => {\n  let offset = 0\n  return utteranceTokens.map(t => {\n    const original = isSpace(t) ? t : utterance.substr(offset, t.length)\n    offset += t.length\n    return original\n  })\n}\n"]}