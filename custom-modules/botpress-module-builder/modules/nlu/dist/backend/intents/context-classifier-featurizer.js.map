{"version":3,"sources":["intents/context-classifier-featurizer.ts"],"names":["shouldConsiterToken","token","isSysOrPatternEntity","entities","some","en","metadata","extractor","isWord","getSentenceEmbeddingForCtx","utt","toks","tokens","filter","_","isEmpty","vector","length","totalWeight","reduce","sum","t","Math","min","tfidf","weightedSum","norm","weightedVec"],"mappings":";;;;;;;AAAA;;AAEA;;;;AAGA,SAASA,mBAAT,CAA6BC,KAA7B,EAA6D;AAC3D,QAAMC,oBAAoB,GAAGD,KAAK,CAACE,QAAN,CAAeC,IAAf,CAC3BC,EAAE,IAAIA,EAAE,CAACC,QAAH,CAAYC,SAAZ,KAA0B,SAA1B,IAAuCF,EAAE,CAACC,QAAH,CAAYC,SAAZ,KAA0B,QAD5C,CAA7B;AAGA,SAAON,KAAK,CAACO,MAAN,IAAgB,CAACN,oBAAxB;AACD;;AAEM,SAASO,0BAAT,CAAoCC,GAApC,EAA8D;AACnE,QAAMC,IAAI,GAAGD,GAAG,CAACE,MAAJ,CAAWC,MAAX,CAAkBb,mBAAlB,CAAb;;AACA,MAAIc,gBAAEC,OAAF,CAAUJ,IAAV,CAAJ,EAAqB;AACnB,WAAO,kBAAOD,GAAG,CAACE,MAAJ,CAAW,CAAX,EAAcI,MAAd,CAAqBC,MAA5B,CAAP;AACD;;AAED,QAAMC,WAAW,GAAGP,IAAI,CAACQ,MAAL,CAAY,CAACC,GAAD,EAAMC,CAAN,KAAYD,GAAG,GAAGE,IAAI,CAACC,GAAL,CAAS,CAAT,EAAYF,CAAC,CAACG,KAAd,CAA9B,EAAoD,CAApD,KAA0D,CAA9E;AACA,QAAMC,WAAW,GAAGd,IAAI,CAACQ,MAAL,CAAY,CAACC,GAAD,EAAMC,CAAN,KAAY;AAC1C,UAAMK,IAAI,GAAG,uBAAsBL,CAAC,CAACL,MAAxB,CAAb;AACA,UAAMW,WAAW,GAAG,wBAAuBN,CAAC,CAACL,MAAzB,EAAiCU,IAAI,GAAGJ,IAAI,CAACC,GAAL,CAAS,CAAT,EAAYF,CAAC,CAACG,KAAd,CAAxC,CAApB;AACA,WAAO,qBAAUJ,GAAV,EAAeO,WAAf,CAAP;AACD,GAJmB,EAIjB,kBAAOjB,GAAG,CAACE,MAAJ,CAAW,CAAX,EAAcI,MAAd,CAAqBC,MAA5B,CAJiB,CAApB;AAMA,SAAO,wBAAaQ,WAAb,EAA0BP,WAA1B,CAAP;AACD","sourceRoot":"/src/modules/nlu/src/backend","sourcesContent":["import _ from 'lodash'\n\nimport { computeNorm, scalarDivide, vectorAdd, zeroes } from '../tools/math'\nimport Utterance, { UtteranceToken } from '../utterance/utterance'\n\nfunction shouldConsiterToken(token: UtteranceToken): boolean {\n  const isSysOrPatternEntity = token.entities.some(\n    en => en.metadata.extractor === 'pattern' || en.metadata.extractor === 'system'\n  )\n  return token.isWord && !isSysOrPatternEntity\n}\n\nexport function getSentenceEmbeddingForCtx(utt: Utterance): number[] {\n  const toks = utt.tokens.filter(shouldConsiterToken)\n  if (_.isEmpty(toks)) {\n    return zeroes(utt.tokens[0].vector.length)\n  }\n\n  const totalWeight = toks.reduce((sum, t) => sum + Math.min(1, t.tfidf), 0) || 1\n  const weightedSum = toks.reduce((sum, t) => {\n    const norm = computeNorm(<number[]>t.vector)\n    const weightedVec = scalarDivide(<number[]>t.vector, norm / Math.min(1, t.tfidf))\n    return vectorAdd(sum, weightedVec)\n  }, zeroes(utt.tokens[0].vector.length))\n\n  return scalarDivide(weightedSum, totalWeight)\n}\n"]}