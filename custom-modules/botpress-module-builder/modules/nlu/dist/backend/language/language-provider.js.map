{"version":3,"sources":["language/language-provider.ts"],"names":["debug","DEBUG","sub","MAX_PAYLOAD_SIZE","JUNK_VOCAB_SIZE","JUNK_TOKEN_MIN","JUNK_TOKEN_MAX","RemoteLanguageProvider","path","join","process","APP_DATA_PATH","interval","max_interval","timeout","max_tries","err","endpoint","logger","status","_","get","details","error","attachError","code","_cacheDumpDisabled","dumpTokensCache","dumpVectorsCache","dumpJunkWordsCache","languages","Object","keys","langs","addProvider","lang","source","client","errors","disabledUntil","undefined","toUpperCase","initialize","sources","_validProvidersCount","_vectorsCache","lru","length","arr","BYTES_PER_ELEMENT","Float32Array","max","_tokensCache","val","key","x","_junkwordsCache","Promise","mapSeries","headers","authToken","proxyConfig","PROXY","httpsAgent","httpsProxyAgent","axios","create","baseURL","data","ready","Error","_languageDims","dimentions","forEach","discoveryRetryPolicy","handleLanguageServerError","restoreVectorsCache","restoreJunkWordsCache","restoreTokensCache","fse","ensureFile","_tokensCachePath","writeJson","dump","message","pathExists","readJSON","load","_vectorsCachePath","writeJSON","kve","map","e","k","v","from","values","_junkwordsCachePath","getHealth","validProvidersCount","validLanguages","getAvailableProviders","filter","Date","queryProvider","body","returnProperty","providers","provider","post","payload","add","toDate","until","generateSimilarJunkWords","subsetVocab","gramset","result","junk","vocab","sim","generateJunkWords","vectorize","set","onJunkWordsCacheChanged","realWords","uniq","meanWordSize","meanBy","w","minJunkSize","Math","maxJunkSize","min","range","finalSize","random","word","sample","tokens","vectors","Array","idxToFetch","getCacheKey","t","encodeURI","token","i","has","push","group","splice","query","idx","toLowerCase","fetched","tokenIdx","fetchIdx","onVectorsCacheChanged","tokenize","utterances","tokenUtterances","utterance","totalSize","sliceUntil","reduce","topIdx","batch","toks","utteranceIdx","onTokensCacheChanged"],"mappings":";;;;;;;AAAA;;AACA;;AAEA;;AACA;;AACA;;AACA;;AACA;;AACA;;AACA;;AAEA;;AACA;;;;;;;;;;AAGA,MAAMA,KAAK,GAAGC,KAAK,CAAC,KAAD,CAAL,CAAaC,GAAb,CAAiB,MAAjB,CAAd;AAEA,MAAMC,gBAAgB,GAAG,MAAM,IAA/B,C,CAAoC;;AACpC,MAAMC,eAAe,GAAG,GAAxB;AACA,MAAMC,cAAc,GAAG,CAAvB;AACA,MAAMC,cAAc,GAAG,EAAvB;;AAEO,MAAMC,sBAAN,CAAyD;AAAA;AAAA,+CAClCC,cAAKC,IAAL,CAAUC,OAAO,CAACC,aAAlB,EAAiC,OAAjC,EAA0C,mBAA1C,CADkC;;AAAA,iDAEhCH,cAAKC,IAAL,CAAUC,OAAO,CAACC,aAAlB,EAAiC,OAAjC,EAA0C,iBAA1C,CAFgC;;AAAA,8CAGnCH,cAAKC,IAAL,CAAUC,OAAO,CAACC,aAAlB,EAAiC,OAAjC,EAA0C,uBAA1C,CAHmC;;AAAA;;AAAA;;AAAA;;AAAA,gDASxB,KATwB;;AAAA;;AAAA;;AAAA,kDAa/B;AAC7BC,MAAAA,QAAQ,EAAE,IADmB;AAE7BC,MAAAA,YAAY,EAAE,IAFe;AAG7BC,MAAAA,OAAO,EAAE,IAHoB;AAI7BC,MAAAA,SAAS,EAAE;AAJkB,KAb+B;;AAAA,mCAoBhC,EApBgC;;AAAA,uDAmH1B,CAACC,GAAD,EAAMC,QAAN,EAAwBC,MAAxB,KAAmC;AACrE,YAAMC,MAAM,GAAGC,gBAAEC,GAAF,CAAML,GAAN,EAAW,yBAAX,CAAf;;AACA,YAAMM,OAAO,GAAGF,gBAAEC,GAAF,CAAML,GAAN,EAAW,0BAAX,CAAhB;;AAEA,UAAIG,MAAM,KAAK,GAAf,EAAoB;AAClBD,QAAAA,MAAM,CAACK,KAAP,CACG,mCAAkCD,OAAQ,mFAAkFL,QAAS,EADxI;AAGD,OAJD,MAIO,IAAIE,MAAM,KAAK,GAAf,EAAoB;AACzBD,QAAAA,MAAM,CAACK,KAAP,CAAc,kEAAiEN,QAAS,EAAxF;AACD,OAFM,MAEA;AACLC,QAAAA,MAAM,CAACM,WAAP,CAAmBR,GAAnB,EAAwBO,KAAxB,CAA+B,uCAAsCN,QAAS,KAAID,GAAG,CAACS,IAAK,EAA3F;AACD;AACF,KAhI6D;;AAAA,kDAkI/B,sBAAS,YAAY;AAClD,UAAI,CAAC,KAAKC,kBAAV,EAA8B;AAC5B,cAAM,KAAKC,eAAL,EAAN;AACD;AACF,KAJ8B,EAI5B,iBAAG,IAAH,CAJ4B,CAlI+B;;AAAA,mDAwI9B,sBAAS,YAAY;AACnD,UAAI,CAAC,KAAKD,kBAAV,EAA8B;AAC5B,cAAM,KAAKE,gBAAL,EAAN;AACD;AACF,KAJ+B,EAI7B,iBAAG,IAAH,CAJ6B,CAxI8B;;AAAA,qDA8I5B,sBAAS,YAAY;AACrD,UAAI,CAAC,KAAKF,kBAAV,EAA8B;AAC5B,cAAM,KAAKG,kBAAL,EAAN;AACD;AACF,KAJiC,EAI/B,iBAAG,IAAH,CAJ+B,CA9I4B;AAAA;;AAsB9D,MAAIC,SAAJ,GAA0B;AACxB,WAAOC,MAAM,CAACC,IAAP,CAAY,KAAKC,KAAjB,CAAP;AACD;;AAEOC,EAAAA,WAAR,CAAoBC,IAApB,EAAkCC,MAAlC,EAA0DC,MAA1D,EAAiF;AAC/E,SAAKJ,KAAL,CAAWE,IAAX,IAAmB,CAAC,IAAI,KAAKF,KAAL,CAAWE,IAAX,KAAoB,EAAxB,CAAD,EAA8B;AAAEC,MAAAA,MAAF;AAAUC,MAAAA,MAAV;AAAkBC,MAAAA,MAAM,EAAE,CAA1B;AAA6BC,MAAAA,aAAa,EAAEC;AAA5C,KAA9B,CAAnB;AACAxC,IAAAA,KAAK,CAAE,IAAGmC,IAAI,CAACM,WAAL,EAAmB,8BAAxB,EAAuDL,MAAvD,CAAL;AACD;;AAED,QAAMM,UAAN,CAAiBC,OAAjB,EAA4CzB,MAA5C,EAAkG;AAChG,SAAK0B,oBAAL,GAA4B,CAA5B;AAEA,SAAKC,aAAL,GAAqB,IAAIC,iBAAJ,CAA8B;AACjDC,MAAAA,MAAM,EAAGC,GAAD,IAAuB;AAC7B,YAAIA,GAAG,IAAIA,GAAG,CAACC,iBAAf,EAAkC;AAChC,iBAAOD,GAAG,CAACD,MAAJ,GAAaC,GAAG,CAACC,iBAAxB;AACD,SAFD,MAEO;AACL,iBAAO;AAAI;AAAJ,YAAgBC,YAAY,CAACD,iBAApC;AACD;AACF,OAPgD;AAQjDE,MAAAA,GAAG,EAAE;AAAI;AAAJ,QAAgBD,YAAY,CAACD;AAAkB;AAA/C,QAA6D;AAAO;;AARxB,KAA9B,CAArB;AAWA,SAAKG,YAAL,GAAoB,IAAIN,iBAAJ,CAA0B;AAC5CC,MAAAA,MAAM,EAAE,CAACM,GAAD,EAAgBC,GAAhB,KAAgCA,GAAG,CAACP,MAAJ,GAAa,CAAb,GAAiB,mBAAMM,GAAN,EAAWE,CAAC,IAAIA,CAAC,CAACR,MAAF,GAAW,CAA3B,CADb;AAE5CI,MAAAA,GAAG,EACD,IAAI;AACJ,OADA,GACI;AACJ,QAFA,GAEK;AACL,QAHA,GAGK;AACL,SAJA,GAIM;AACN,QALA,GAKK;AACL,QAT0C,CASvC;AACL;;AAV4C,KAA1B,CAApB;AAaA,SAAKK,eAAL,GAAuB,IAAIV,iBAAJ,CAA4B;AACjDC,MAAAA,MAAM,EAAE,CAACM,GAAD,EAAgBC,GAAhB,KAAkC,mBAAMA,GAAN,EAAWC,CAAC,IAAIA,CAAC,CAACR,MAAF,GAAW,CAA3B,IAAgC,mBAAMM,GAAN,EAAWE,CAAC,IAAIA,CAAC,CAACR,MAAF,GAAW,CAA3B,CADzB;AAEjDI,MAAAA,GAAG,EACD,IAAI;AACJ,QADA,GACK;AACL,SAFA,GAEM;AACN,UAHA,GAGO;AACP,QAP+C,CAO5C;AACL;;AARiD,KAA5B,CAAvB;AAWA,UAAMM,OAAO,CAACC,SAAR,CAAkBf,OAAlB,EAA2B,MAAMP,MAAN,IAAgB;AAC/C,YAAMuB,OAAO,GAAG,EAAhB;;AAEA,UAAIvB,MAAM,CAACwB,SAAX,EAAsB;AACpBD,QAAAA,OAAO,CAAC,eAAD,CAAP,GAA2B,YAAYvB,MAAM,CAACwB,SAA9C;AACD;;AAED,YAAMC,WAAW,GAAGnD,OAAO,CAACoD,KAAR,GAAgB;AAAEC,QAAAA,UAAU,EAAE,IAAIC,wBAAJ,CAAoBtD,OAAO,CAACoD,KAA5B;AAAd,OAAhB,GAAqE,EAAzF;;AAEA,YAAMzB,MAAM,GAAG4B,eAAMC,MAAN,CAAa;AAC1BC,QAAAA,OAAO,EAAE/B,MAAM,CAACnB,QADU;AAE1B0C,QAAAA,OAF0B;AAG1B,WAAGE;AAHuB,OAAb,CAAf;;AAKA,UAAI;AACF,cAAM,4BAAM,YAAY;AACtB,gBAAM;AAAEO,YAAAA;AAAF,cAAW,MAAM/B,MAAM,CAAChB,GAAP,CAAW,OAAX,CAAvB;;AAEA,cAAI,CAAC+C,IAAI,CAACC,KAAV,EAAiB;AACf,kBAAM,IAAIC,KAAJ,CAAU,8BAAV,CAAN;AACD;;AAED,cAAI,CAAC,KAAKC,aAAV,EAAyB;AACvB,iBAAKA,aAAL,GAAqBH,IAAI,CAACI,UAA1B,CADuB,CACc;AACtC;;AAED,cAAI,KAAKD,aAAL,KAAuBH,IAAI,CAACI,UAAhC,EAA4C;AAC1C,kBAAM,IAAIF,KAAJ,CAAU,4CAAV,CAAN;AACD;;AACD,eAAK1B,oBAAL;AACAwB,UAAAA,IAAI,CAACtC,SAAL,CAAe2C,OAAf,CAAuBlB,CAAC,IAAI,KAAKrB,WAAL,CAAiBqB,CAAC,CAACpB,IAAnB,EAAyBC,MAAzB,EAAiCC,MAAjC,CAA5B;AACD,SAhBK,EAgBH,KAAKqC,oBAhBF,CAAN;AAiBD,OAlBD,CAkBE,OAAO1D,GAAP,EAAY;AACZ,aAAK2D,yBAAL,CAA+B3D,GAA/B,EAAoCoB,MAAM,CAACnB,QAA3C,EAAqDC,MAArD;AACD;AACF,KAnCK,CAAN;AAqCAlB,IAAAA,KAAK,CAAE,UAAS+B,MAAM,CAACC,IAAP,CAAY,KAAKC,KAAjB,EAAwBc,MAAO,mBAAkBJ,OAAO,CAACI,MAAO,UAA3E,CAAL;AAEA,UAAM,KAAK6B,mBAAL,EAAN;AACA,UAAM,KAAKC,qBAAL,EAAN;AACA,UAAM,KAAKC,kBAAL,EAAN;AAEA,WAAO,IAAP;AACD;;AAmCD,QAAcnD,eAAd,GAAgC;AAC9B,QAAI;AACF,YAAMoD,iBAAIC,UAAJ,CAAe,KAAKC,gBAApB,CAAN;AACA,YAAMF,iBAAIG,SAAJ,CAAc,KAAKD,gBAAnB,EAAqC,KAAK7B,YAAL,CAAkB+B,IAAlB,EAArC,CAAN;AACAnF,MAAAA,KAAK,CAAC,6BAAD,EAAgC,KAAKiF,gBAArC,CAAL;AACD,KAJD,CAIE,OAAOjE,GAAP,EAAY;AACZhB,MAAAA,KAAK,CAAC,2CAAD,EAA8CgB,GAAG,CAACoE,OAAlD,CAAL;AACA,WAAK1D,kBAAL,GAA0B,IAA1B;AACD;AACF;;AAED,QAAcoD,kBAAd,GAAmC;AACjC,QAAI;AACF,UAAI,MAAMC,iBAAIM,UAAJ,CAAe,KAAKJ,gBAApB,CAAV,EAAiD;AAC/C,cAAME,IAAI,GAAG,MAAMJ,iBAAIO,QAAJ,CAAa,KAAKL,gBAAlB,CAAnB;;AACA,aAAK7B,YAAL,CAAkBmC,IAAlB,CAAuBJ,IAAvB;AACD;AACF,KALD,CAKE,OAAOnE,GAAP,EAAY;AACZhB,MAAAA,KAAK,CAAC,2CAAD,EAA8CgB,GAAG,CAACoE,OAAlD,CAAL;AACD;AACF;;AAED,QAAcxD,gBAAd,GAAiC;AAC/B,QAAI;AACF,YAAMmD,iBAAIC,UAAJ,CAAe,KAAKQ,iBAApB,CAAN;AACA,YAAMT,iBAAIU,SAAJ,CAAc,KAAKD,iBAAnB,EAAsC,KAAK3C,aAAL,CAAmBsC,IAAnB,EAAtC,CAAN;AACAnF,MAAAA,KAAK,CAAC,8BAAD,EAAiC,KAAKwF,iBAAtC,CAAL;AACD,KAJD,CAIE,OAAOxE,GAAP,EAAY;AACZhB,MAAAA,KAAK,CAAC,4CAAD,EAA+CgB,GAAG,CAACoE,OAAnD,CAAL;AACA,WAAK1D,kBAAL,GAA0B,IAA1B;AACD;AACF;;AAED,QAAckD,mBAAd,GAAoC;AAClC,QAAI;AACF,UAAI,MAAMG,iBAAIM,UAAJ,CAAe,KAAKG,iBAApB,CAAV,EAAkD;AAChD,cAAML,IAAI,GAAG,MAAMJ,iBAAIO,QAAJ,CAAa,KAAKE,iBAAlB,CAAnB;;AACA,YAAIL,IAAJ,EAAU;AACR,gBAAMO,GAAG,GAAGP,IAAI,CAACQ,GAAL,CAASpC,CAAC,KAAK;AAAEqC,YAAAA,CAAC,EAAErC,CAAC,CAACqC,CAAP;AAAUC,YAAAA,CAAC,EAAEtC,CAAC,CAACsC,CAAf;AAAkBC,YAAAA,CAAC,EAAE5C,YAAY,CAAC6C,IAAb,CAAkBhE,MAAM,CAACiE,MAAP,CAAczC,CAAC,CAACuC,CAAhB,CAAlB;AAArB,WAAL,CAAV,CAAZ;;AACA,eAAKjD,aAAL,CAAmB0C,IAAnB,CAAwBG,GAAxB;AACD;AACF;AACF,KARD,CAQE,OAAO1E,GAAP,EAAY;AACZhB,MAAAA,KAAK,CAAC,4CAAD,EAA+CgB,GAAG,CAACoE,OAAnD,CAAL;AACD;AACF;;AAED,QAAcvD,kBAAd,GAAmC;AACjC,QAAI;AACF,YAAMkD,iBAAIC,UAAJ,CAAe,KAAKiB,mBAApB,CAAN;AACA,YAAMlB,iBAAIU,SAAJ,CAAc,KAAKQ,mBAAnB,EAAwC,KAAKzC,eAAL,CAAqB2B,IAArB,EAAxC,CAAN;AACAnF,MAAAA,KAAK,CAAC,iCAAD,EAAoC,KAAKwD,eAAzC,CAAL;AACD,KAJD,CAIE,OAAOxC,GAAP,EAAY;AACZhB,MAAAA,KAAK,CAAC,yCAAD,EAA4CgB,GAAG,CAACoE,OAAhD,CAAL;AACA,WAAK1D,kBAAL,GAA0B,IAA1B;AACD;AACF;;AAED,QAAcmD,qBAAd,GAAsC;AACpC,QAAI;AACF,UAAI,MAAME,iBAAIM,UAAJ,CAAe,KAAKY,mBAApB,CAAV,EAAoD;AAClD,cAAMd,IAAI,GAAG,MAAMJ,iBAAIO,QAAJ,CAAa,KAAKW,mBAAlB,CAAnB;;AACA,aAAKpD,aAAL,CAAmB0C,IAAnB,CAAwBJ,IAAxB;AACD;AACF,KALD,CAKE,OAAOnE,GAAP,EAAY;AACZhB,MAAAA,KAAK,CAAC,yCAAD,EAA4CgB,GAAG,CAACoE,OAAhD,CAAL;AACD;AACF;;AAEDc,EAAAA,SAAS,GAAuB;AAC9B,WAAO;AAAEC,MAAAA,mBAAmB,EAAE,KAAKvD,oBAA5B;AAAkDwD,MAAAA,cAAc,EAAErE,MAAM,CAACC,IAAP,CAAY,KAAKC,KAAjB;AAAlE,KAAP;AACD;;AAEOoE,EAAAA,qBAAR,CAA8BlE,IAA9B,EAAuD;AACrD,QAAI,CAAC,KAAKF,KAAL,CAAWE,IAAX,CAAL,EAAuB;AACrB,YAAM,IAAImC,KAAJ,CAAW,aAAYnC,IAAK,uDAA5B,CAAN;AACD;;AAED,WAAO,KAAKF,KAAL,CAAWE,IAAX,EAAiBmE,MAAjB,CAAwB/C,CAAC,IAAI,CAACA,CAAC,CAAChB,aAAH,IAAoBgB,CAAC,CAAChB,aAAF,IAAmB,IAAIgE,IAAJ,EAApE,CAAP;AACD;;AAED,QAAcC,aAAd,CAA+BrE,IAA/B,EAA6C3B,IAA7C,EAA2DiG,IAA3D,EAAsEC,cAAtE,EAA0G;AACxG,UAAMC,SAAS,GAAG,KAAKN,qBAAL,CAA2BlE,IAA3B,CAAlB;;AAEA,SAAK,MAAMyE,QAAX,IAAuBD,SAAvB,EAAkC;AAChC,UAAI;AACF,cAAM;AAAEvC,UAAAA;AAAF,YAAW,MAAMwC,QAAQ,CAACvE,MAAT,CAAgBwE,IAAhB,CAAqBrG,IAArB,EAA2B,EAAE,GAAGiG,IAAL;AAAWtE,UAAAA;AAAX,SAA3B,CAAvB;;AAEA,YAAIiC,IAAI,IAAIA,IAAI,CAACsC,cAAD,CAAhB,EAAkC;AAChC,iBAAOtC,IAAI,CAACsC,cAAD,CAAX;AACD;;AAED,eAAOtC,IAAP;AACD,OARD,CAQE,OAAOpD,GAAP,EAAY;AACZhB,QAAAA,KAAK,CAAC,4BAAD,EAA+B;AAAEoF,UAAAA,OAAO,EAAEpE,GAAG,CAACoE,OAAf;AAAwB3D,UAAAA,IAAI,EAAET,GAAG,CAACS,IAAlC;AAAwCN,UAAAA,MAAM,EAAEH,GAAG,CAACG,MAApD;AAA4D2F,UAAAA,OAAO,EAAEL;AAArE,SAA/B,CAAL;;AAEA,YAAI,KAAKJ,qBAAL,CAA2BlE,IAA3B,EAAiCY,MAAjC,GAA0C,CAA9C,EAAiD;AAC/C;AACA6D,UAAAA,QAAQ,CAACrE,aAAT,GAAyB,uBACtBwE,GADsB,CAClBH,QAAQ,CAACtE,MAAT,EADkB,EACC,SADD,EAEtB0E,MAFsB,EAAzB;AAIAhH,UAAAA,KAAK,CAAC,6BAAD,EAAgC;AACnCoC,YAAAA,MAAM,EAAEwE,QAAQ,CAACxE,MADkB;AAEnCpB,YAAAA,GAAG,EAAEA,GAAG,CAACoE,OAF0B;AAGnC9C,YAAAA,MAAM,EAAEsE,QAAQ,CAACtE,MAHkB;AAInC2E,YAAAA,KAAK,EAAEL,QAAQ,CAACrE;AAJmB,WAAhC,CAAL;AAMD;AACF;AACF;;AAED,UAAM,IAAI+B,KAAJ,CAAW,mDAAkD9D,IAAK,eAAc2B,IAAK,GAArF,CAAN;AACD;AAED;;;;;;AAIA,QAAM+E,wBAAN,CAA+BC,WAA/B,EAAsDhF,IAAtD,EAAuF;AACrF;AACA;AACA;AACA,UAAMiF,OAAO,GAAG,yBAAWD,WAAX,CAAhB;AACA,QAAIE,MAAJ;;AAEA,SAAK7D,eAAL,CAAqBiB,OAArB,CAA6B,CAAC6C,IAAD,EAAOC,KAAP,KAAiB;AAC5C,UAAI,CAACF,MAAL,EAAa;AACX,cAAMG,GAAG,GAAG,4BAAcD,KAAd,EAAqBH,OAArB,CAAZ;;AACA,YAAII,GAAG,IAAI,IAAX,EAAiB;AACfH,UAAAA,MAAM,GAAGC,IAAT;AACD;AACF;AACF,KAPD;;AASA,QAAI,CAACD,MAAL,EAAa;AACX;AACAA,MAAAA,MAAM,GAAG,KAAKI,iBAAL,CAAuBN,WAAvB,EAAoCC,OAApC,CAAT,CAFW,CAE2C;;AACtD,YAAM,KAAKM,SAAL,CAAeL,MAAf,EAAuBlF,IAAvB,CAAN,CAHW,CAGwB;;AACnC,WAAKqB,eAAL,CAAqBmE,GAArB,CAAyBP,OAAzB,EAAkCC,MAAlC;;AACA,YAAM,KAAKO,uBAAL,EAAN;AACD;;AAED,WAAOP,MAAP;AACD;;AAEOI,EAAAA,iBAAR,CAA0BN,WAA1B,EAAiDC,OAAjD,EAAoE;AAClE,UAAMS,SAAS,GAAGzG,gBAAE0G,IAAF,CAAOX,WAAP,CAAlB;;AACA,UAAMY,YAAY,GAAG3G,gBAAE4G,MAAF,CAASH,SAAT,EAAoBI,CAAC,IAAIA,CAAC,CAAClF,MAA3B,CAArB;;AACA,UAAMmF,WAAW,GAAGC,IAAI,CAAChF,GAAL,CAAS9C,cAAT,EAAyB0H,YAAY,GAAG,CAAxC,CAApB,CAHkE,CAGH;;AAC/D,UAAMK,WAAW,GAAGD,IAAI,CAACE,GAAL,CAAS/H,cAAT,EAAyByH,YAAY,GAAG,GAAxC,CAApB,CAJkE,CAID;;AACjE,WAAO3G,gBAAEkH,KAAF,CAAQ,CAAR,EAAWlI,eAAX,EAA4BuF,GAA5B,CAAgC,MAAM;AAC3C,YAAM4C,SAAS,GAAGnH,gBAAEoH,MAAF,CAASN,WAAT,EAAsBE,WAAtB,EAAmC,KAAnC,CAAlB;;AACA,UAAIK,IAAI,GAAG,EAAX;;AACA,aAAOA,IAAI,CAAC1F,MAAL,GAAcwF,SAArB,EAAgC;AAC9BE,QAAAA,IAAI,IAAIrH,gBAAEsH,MAAF,CAAStB,OAAT,CAAR;AACD;;AACD,aAAOqB,IAAP;AACD,KAPM,CAAP,CALkE,CAY/D;AACJ;;AAED,QAAMf,SAAN,CAAgBiB,MAAhB,EAAkCxG,IAAlC,EAAyE;AACvE,QAAI,CAACwG,MAAM,CAAC5F,MAAZ,EAAoB;AAClB,aAAO,EAAP;AACD;;AAED,UAAM6F,OAAuB,GAAGC,KAAK,CAACF,MAAM,CAAC5F,MAAR,CAArC;AACA,UAAM+F,UAAoB,GAAG,EAA7B,CANuE,CAMvC;;AAChC,UAAMC,WAAW,GAAIC,CAAD,IAAgB,GAAE7G,IAAK,IAAG8G,SAAS,CAACD,CAAD,CAAI,EAA3D;;AAEAL,IAAAA,MAAM,CAAClE,OAAP,CAAe,CAACyE,KAAD,EAAQC,CAAR,KAAc;AAC3B,UAAI,yBAAQD,KAAR,CAAJ,EAAoB;AAClBN,QAAAA,OAAO,CAACO,CAAD,CAAP,GAAa,IAAIjG,YAAJ,CAAiB,KAAKqB,aAAtB,CAAb,CADkB,CACgC;AACnD,OAFD,MAEO,IAAI,KAAK1B,aAAL,CAAmBuG,GAAnB,CAAuBL,WAAW,CAACG,KAAD,CAAlC,CAAJ,EAAgD;AACrDN,QAAAA,OAAO,CAACO,CAAD,CAAP,GAAa,KAAKtG,aAAL,CAAmBxB,GAAnB,CAAuB0H,WAAW,CAACG,KAAD,CAAlC,CAAb;AACD,OAFM,MAEA;AACLJ,QAAAA,UAAU,CAACO,IAAX,CAAgBF,CAAhB;AACD;AACF,KARD;;AAUA,WAAOL,UAAU,CAAC/F,MAAlB,EAA0B;AACxB;AACA,YAAMuG,KAAK,GAAGR,UAAU,CAACS,MAAX,CAAkB,CAAlB,EAAqB,GAArB,CAAd,CAFwB,CAIxB;;AACA,YAAMC,KAAK,GAAGF,KAAK,CAAC3D,GAAN,CAAU8D,GAAG,IAAId,MAAM,CAACc,GAAD,CAAN,CAAYC,WAAZ,EAAjB,CAAd,CALwB,CAMxB;;AACA,UAAI,CAACF,KAAK,CAACzG,MAAX,EAAmB;AACjB;AACD;;AAED,YAAM4G,OAAO,GAAG,MAAM,KAAKnD,aAAL,CAA+BrE,IAA/B,EAAqC,YAArC,EAAmD;AAAEwG,QAAAA,MAAM,EAAEa;AAAV,OAAnD,EAAsE,SAAtE,CAAtB;;AAEA,UAAIG,OAAO,CAAC5G,MAAR,KAAmByG,KAAK,CAACzG,MAA7B,EAAqC;AACnC,cAAM,IAAIuB,KAAJ,CACH,2EAA0EkF,KAAK,CAACzG,MAAO,iBAAgB4G,OAAO,CAAC5G,MAAO,GADnH,CAAN;AAGD,OAjBuB,CAmBxB;;;AACAuG,MAAAA,KAAK,CAAC7E,OAAN,CAAc,CAACmF,QAAD,EAAWC,QAAX,KAAwB;AACpCjB,QAAAA,OAAO,CAACgB,QAAD,CAAP,GAAoB1G,YAAY,CAAC6C,IAAb,CAAkB4D,OAAO,CAACE,QAAD,CAAzB,CAApB;;AACA,aAAKhH,aAAL,CAAmB8E,GAAnB,CAAuBoB,WAAW,CAACJ,MAAM,CAACiB,QAAD,CAAP,CAAlC,EAAsDhB,OAAO,CAACgB,QAAD,CAA7D;AACD,OAHD;AAKA,YAAM,KAAKE,qBAAL,EAAN;AACD;;AAED,WAAOlB,OAAP;AACD;;AAED,QAAMmB,QAAN,CAAeC,UAAf,EAAqC7H,IAArC,EAAmDoF,KAAgB,GAAG,EAAtE,EAA+F;AAC7F,QAAI,CAACyC,UAAU,CAACjH,MAAhB,EAAwB;AACtB,aAAO,EAAP;AACD;;AAED,UAAMgG,WAAW,GAAIC,CAAD,IAAgB,GAAE7G,IAAK,IAAG8G,SAAS,CAACD,CAAD,CAAI,EAA3D;;AACA,UAAMiB,eAA2B,GAAGpB,KAAK,CAACmB,UAAU,CAACjH,MAAZ,CAAzC;AACA,UAAM+F,UAAoB,GAAG,EAA7B,CAP6F,CAO7D;;AAEhCkB,IAAAA,UAAU,CAACvF,OAAX,CAAmB,CAACyF,SAAD,EAAYT,GAAZ,KAAoB;AACrC,UAAI,KAAKrG,YAAL,CAAkBgG,GAAlB,CAAsBL,WAAW,CAACmB,SAAD,CAAjC,CAAJ,EAAmD;AACjDD,QAAAA,eAAe,CAACR,GAAD,CAAf,GAAuB,KAAKrG,YAAL,CAAkB/B,GAAlB,CAAsB0H,WAAW,CAACmB,SAAD,CAAjC,CAAvB;AACD,OAFD,MAEO;AACLpB,QAAAA,UAAU,CAACO,IAAX,CAAgBI,GAAhB;AACD;AACF,KAND,EAT6F,CAiB7F;AACA;;AAEA,WAAOX,UAAU,CAAC/F,MAAlB,EAA0B;AACxB;AACA;AACA,UAAIoH,SAAS,GAAG,CAAhB;AACA,YAAMC,UAAU,GAAGtB,UAAU,CAACuB,MAAX,CAAkB,CAACC,MAAD,EAASb,GAAT,EAAcN,CAAd,KAAoB;AACvD,YAAI,CAACgB,SAAS,IAAIH,UAAU,CAACP,GAAD,CAAV,CAAgB1G,MAAhB,GAAyB,CAAvC,IAA4C5C,gBAAhD,EAAkE;AAChE,iBAAOgJ,CAAP;AACD,SAFD,MAEO;AACL,iBAAOmB,MAAP;AACD;AACF,OANkB,EAMhB,CANgB,CAAnB;AAOA,YAAMC,KAAK,GAAGzB,UAAU,CAACS,MAAX,CAAkB,CAAlB,EAAqBa,UAAU,GAAG,CAAlC,CAAd;AACA,YAAMZ,KAAK,GAAGe,KAAK,CAAC5E,GAAN,CAAU8D,GAAG,IAAIO,UAAU,CAACP,GAAD,CAAV,CAAgBC,WAAhB,EAAjB,CAAd;;AAEA,UAAI,CAACF,KAAK,CAACzG,MAAX,EAAmB;AACjB;AACD;;AAED,UAAI4G,OAAO,GAAG,MAAM,KAAKnD,aAAL,CAA+BrE,IAA/B,EAAqC,WAArC,EAAkD;AAAE6H,QAAAA,UAAU,EAAER;AAAd,OAAlD,EAAyE,QAAzE,CAApB;AACAG,MAAAA,OAAO,GAAGA,OAAO,CAAChE,GAAR,CAAY6E,IAAI,IAAI,wCAAuBA,IAAvB,EAA6BjD,KAA7B,CAApB,CAAV;;AAEA,UAAIoC,OAAO,CAAC5G,MAAR,KAAmByG,KAAK,CAACzG,MAA7B,EAAqC;AACnC,cAAM,IAAIuB,KAAJ,CACH,8EAA6EkF,KAAK,CAACzG,MAAO,iBAAgB4G,OAAO,CAAC5G,MAAO,GADtH,CAAN;AAGD,OAzBuB,CA2BxB;;;AACAwH,MAAAA,KAAK,CAAC9F,OAAN,CAAc,CAACgG,YAAD,EAAeZ,QAAf,KAA4B;AACxCI,QAAAA,eAAe,CAACQ,YAAD,CAAf,GAAgC5B,KAAK,CAAC9C,IAAN,CAAW4D,OAAO,CAACE,QAAD,CAAlB,CAAhC;;AACA,aAAKzG,YAAL,CAAkBuE,GAAlB,CAAsBoB,WAAW,CAACiB,UAAU,CAACS,YAAD,CAAX,CAAjC,EAA6DR,eAAe,CAACQ,YAAD,CAA5E;AACD,OAHD;AAKA,YAAM,KAAKC,oBAAL,EAAN;AACD,KAtD4F,CAwD7F;;;AACA,WAAOT,eAAe,CAACtE,GAAhB,CAAoB,CAACgD,MAAD,EAASQ,CAAT,KAAe,gDAA+BR,MAA/B,EAAuCqB,UAAU,CAACb,CAAD,CAAjD,CAAnC,CAAP;AACD;;AAja6D;;;;eAoajD,IAAI5I,sBAAJ,E","sourceRoot":"/src/modules/nlu/src/backend","sourcesContent":["import axios, { AxiosInstance } from 'axios'\nimport retry from 'bluebird-retry'\nimport * as sdk from 'botpress/sdk'\nimport fse from 'fs-extra'\nimport httpsProxyAgent from 'https-proxy-agent'\nimport _, { debounce, sumBy } from 'lodash'\nimport lru from 'lru-cache'\nimport moment from 'moment'\nimport ms from 'ms'\nimport path from 'path'\n\nimport { setSimilarity, vocabNGram } from '../tools/strings'\nimport { isSpace, processUtteranceTokens, restoreOriginalUtteranceCasing } from '../tools/token-utils'\nimport { Gateway, LangsGateway, LanguageProvider, LanguageSource, NLUHealth, Token2Vec } from '../typings'\n\nconst debug = DEBUG('nlu').sub('lang')\n\nconst MAX_PAYLOAD_SIZE = 150 * 1024 // 150kb\nconst JUNK_VOCAB_SIZE = 500\nconst JUNK_TOKEN_MIN = 1\nconst JUNK_TOKEN_MAX = 20\n\nexport class RemoteLanguageProvider implements LanguageProvider {\n  private _vectorsCachePath = path.join(process.APP_DATA_PATH, 'cache', 'lang_vectors.json')\n  private _junkwordsCachePath = path.join(process.APP_DATA_PATH, 'cache', 'junk_words.json')\n  private _tokensCachePath = path.join(process.APP_DATA_PATH, 'cache', 'utterance_tokens.json')\n\n  private _vectorsCache: lru<string, Float32Array>\n  private _tokensCache: lru<string, string[]>\n  private _junkwordsCache: lru<string[], string[]>\n\n  private _cacheDumpDisabled: boolean = false\n  private _validProvidersCount: number\n  private _languageDims: number\n\n  private discoveryRetryPolicy = {\n    interval: 1000,\n    max_interval: 5000,\n    timeout: 2000,\n    max_tries: 5\n  }\n\n  private langs: LangsGateway = {}\n\n  get languages(): string[] {\n    return Object.keys(this.langs)\n  }\n\n  private addProvider(lang: string, source: LanguageSource, client: AxiosInstance) {\n    this.langs[lang] = [...(this.langs[lang] || []), { source, client, errors: 0, disabledUntil: undefined }]\n    debug(`[${lang.toUpperCase()}] Language Provider added %o`, source)\n  }\n\n  async initialize(sources: LanguageSource[], logger: typeof sdk.logger): Promise<LanguageProvider> {\n    this._validProvidersCount = 0\n\n    this._vectorsCache = new lru<string, Float32Array>({\n      length: (arr: Float32Array) => {\n        if (arr && arr.BYTES_PER_ELEMENT) {\n          return arr.length * arr.BYTES_PER_ELEMENT\n        } else {\n          return 300 /* dim */ * Float32Array.BYTES_PER_ELEMENT\n        }\n      },\n      max: 300 /* dim */ * Float32Array.BYTES_PER_ELEMENT /* bytes */ * 500000 /* tokens */\n    })\n\n    this._tokensCache = new lru<string, string[]>({\n      length: (val: string[], key: string) => key.length * 4 + sumBy(val, x => x.length * 4),\n      max:\n        4 * // bytes in strings\n        5 * // average size of token\n        10 * // nb of tokens per utterance\n        10 * // nb of utterances per intent\n        200 * // nb of intents per model\n        10 * // nb of models per bot\n        50 // nb of bots\n      // total is ~ 200 mb\n    })\n\n    this._junkwordsCache = new lru<string[], string[]>({\n      length: (val: string[], key: string[]) => sumBy(key, x => x.length * 4) + sumBy(val, x => x.length * 4),\n      max:\n        4 * // bytes in strings\n        10 * // token size\n        500 * // vocab size\n        1000 * // junk words\n        10 // models\n      // total is ~ 200 mb\n    })\n\n    await Promise.mapSeries(sources, async source => {\n      const headers = {}\n\n      if (source.authToken) {\n        headers['authorization'] = 'bearer ' + source.authToken\n      }\n\n      const proxyConfig = process.PROXY ? { httpsAgent: new httpsProxyAgent(process.PROXY) } : {}\n\n      const client = axios.create({\n        baseURL: source.endpoint,\n        headers,\n        ...proxyConfig\n      })\n      try {\n        await retry(async () => {\n          const { data } = await client.get('/info')\n\n          if (!data.ready) {\n            throw new Error('Language source is not ready')\n          }\n\n          if (!this._languageDims) {\n            this._languageDims = data.dimentions // note typo in language server\n          }\n\n          if (this._languageDims !== data.dimentions) {\n            throw new Error('Language sources have different dimensions')\n          }\n          this._validProvidersCount++\n          data.languages.forEach(x => this.addProvider(x.lang, source, client))\n        }, this.discoveryRetryPolicy)\n      } catch (err) {\n        this.handleLanguageServerError(err, source.endpoint, logger)\n      }\n    })\n\n    debug(`loaded ${Object.keys(this.langs).length} languages from ${sources.length} sources`)\n\n    await this.restoreVectorsCache()\n    await this.restoreJunkWordsCache()\n    await this.restoreTokensCache()\n\n    return this\n  }\n\n  private handleLanguageServerError = (err, endpoint: string, logger) => {\n    const status = _.get(err, 'failure.response.status')\n    const details = _.get(err, 'failure.response.message')\n\n    if (status === 429) {\n      logger.error(\n        `Could not load Language Server: ${details}. You may be over the limit for the number of requests allowed for the endpoint ${endpoint}`\n      )\n    } else if (status === 401) {\n      logger.error(`You must provide a valid authentication token for the endpoint ${endpoint}`)\n    } else {\n      logger.attachError(err).error(`Could not load Language Provider at ${endpoint}: ${err.code}`)\n    }\n  }\n\n  private onTokensCacheChanged = debounce(async () => {\n    if (!this._cacheDumpDisabled) {\n      await this.dumpTokensCache()\n    }\n  }, ms('5s'))\n\n  private onVectorsCacheChanged = debounce(async () => {\n    if (!this._cacheDumpDisabled) {\n      await this.dumpVectorsCache()\n    }\n  }, ms('5s'))\n\n  private onJunkWordsCacheChanged = debounce(async () => {\n    if (!this._cacheDumpDisabled) {\n      await this.dumpJunkWordsCache()\n    }\n  }, ms('5s'))\n\n  private async dumpTokensCache() {\n    try {\n      await fse.ensureFile(this._tokensCachePath)\n      await fse.writeJson(this._tokensCachePath, this._tokensCache.dump())\n      debug('tokens cache updated at: %s', this._tokensCachePath)\n    } catch (err) {\n      debug('could not persist tokens cache, error: %s', err.message)\n      this._cacheDumpDisabled = true\n    }\n  }\n\n  private async restoreTokensCache() {\n    try {\n      if (await fse.pathExists(this._tokensCachePath)) {\n        const dump = await fse.readJSON(this._tokensCachePath)\n        this._tokensCache.load(dump)\n      }\n    } catch (err) {\n      debug('could not restore tokens cache, error: %s', err.message)\n    }\n  }\n\n  private async dumpVectorsCache() {\n    try {\n      await fse.ensureFile(this._vectorsCachePath)\n      await fse.writeJSON(this._vectorsCachePath, this._vectorsCache.dump())\n      debug('vectors cache updated at: %s', this._vectorsCachePath)\n    } catch (err) {\n      debug('could not persist vectors cache, error: %s', err.message)\n      this._cacheDumpDisabled = true\n    }\n  }\n\n  private async restoreVectorsCache() {\n    try {\n      if (await fse.pathExists(this._vectorsCachePath)) {\n        const dump = await fse.readJSON(this._vectorsCachePath)\n        if (dump) {\n          const kve = dump.map(x => ({ e: x.e, k: x.k, v: Float32Array.from(Object.values(x.v)) }))\n          this._vectorsCache.load(kve)\n        }\n      }\n    } catch (err) {\n      debug('could not restore vectors cache, error: %s', err.message)\n    }\n  }\n\n  private async dumpJunkWordsCache() {\n    try {\n      await fse.ensureFile(this._junkwordsCachePath)\n      await fse.writeJSON(this._junkwordsCachePath, this._junkwordsCache.dump())\n      debug('junk words cache updated at: %s', this._junkwordsCache)\n    } catch (err) {\n      debug('could not persist junk cache, error: %s', err.message)\n      this._cacheDumpDisabled = true\n    }\n  }\n\n  private async restoreJunkWordsCache() {\n    try {\n      if (await fse.pathExists(this._junkwordsCachePath)) {\n        const dump = await fse.readJSON(this._junkwordsCachePath)\n        this._vectorsCache.load(dump)\n      }\n    } catch (err) {\n      debug('could not restore junk cache, error: %s', err.message)\n    }\n  }\n\n  getHealth(): Partial<NLUHealth> {\n    return { validProvidersCount: this._validProvidersCount, validLanguages: Object.keys(this.langs) }\n  }\n\n  private getAvailableProviders(lang: string): Gateway[] {\n    if (!this.langs[lang]) {\n      throw new Error(`Language \"${lang}\" is not supported by the configured language sources`)\n    }\n\n    return this.langs[lang].filter(x => !x.disabledUntil || x.disabledUntil <= new Date())\n  }\n\n  private async queryProvider<T>(lang: string, path: string, body: any, returnProperty: string): Promise<T> {\n    const providers = this.getAvailableProviders(lang)\n\n    for (const provider of providers) {\n      try {\n        const { data } = await provider.client.post(path, { ...body, lang })\n\n        if (data && data[returnProperty]) {\n          return data[returnProperty] as T\n        }\n\n        return data\n      } catch (err) {\n        debug('error from language server', { message: err.message, code: err.code, status: err.status, payload: body })\n\n        if (this.getAvailableProviders(lang).length > 1) {\n          // we don't disable providers when there's no backup\n          provider.disabledUntil = moment()\n            .add(provider.errors++, 'seconds')\n            .toDate()\n\n          debug('disabled temporarily source', {\n            source: provider.source,\n            err: err.message,\n            errors: provider.errors,\n            until: provider.disabledUntil\n          })\n        }\n      }\n    }\n\n    throw new Error(`No provider could successfully fullfil request \"${path}\" for lang \"${lang}\"`)\n  }\n\n  /**\n   * Generates words that don't exist in the vocabulary, but that are built from ngrams of existing vocabulary\n   * @param subsetVocab The tokens to which you want similar tokens to\n   */\n  async generateSimilarJunkWords(subsetVocab: string[], lang: string): Promise<string[]> {\n    // TODO: we can remove await + lang\n    // from totalVocab compute the cachedKey the closest to what we have\n    // if 75% of the vocabulary is the same, we keep the cache we have instead of rebuilding one\n    const gramset = vocabNGram(subsetVocab)\n    let result: string[] | undefined\n\n    this._junkwordsCache.forEach((junk, vocab) => {\n      if (!result) {\n        const sim = setSimilarity(vocab, gramset)\n        if (sim >= 0.75) {\n          result = junk\n        }\n      }\n    })\n\n    if (!result) {\n      // didn't find any close gramset, let's create a new one\n      result = this.generateJunkWords(subsetVocab, gramset) // randomly generated words\n      await this.vectorize(result, lang) // vectorize them all in one request to cache the tokens // TODO: remove this\n      this._junkwordsCache.set(gramset, result)\n      await this.onJunkWordsCacheChanged()\n    }\n\n    return result\n  }\n\n  private generateJunkWords(subsetVocab: string[], gramset: string[]) {\n    const realWords = _.uniq(subsetVocab)\n    const meanWordSize = _.meanBy(realWords, w => w.length)\n    const minJunkSize = Math.max(JUNK_TOKEN_MIN, meanWordSize / 2) // Twice as short\n    const maxJunkSize = Math.min(JUNK_TOKEN_MAX, meanWordSize * 1.5) // A bit longer.  Those numbers are discretionary and are not expected to make a big impact on the models.\n    return _.range(0, JUNK_VOCAB_SIZE).map(() => {\n      const finalSize = _.random(minJunkSize, maxJunkSize, false)\n      let word = ''\n      while (word.length < finalSize) {\n        word += _.sample(gramset)\n      }\n      return word\n    }) // randomly generated words\n  }\n\n  async vectorize(tokens: string[], lang: string): Promise<Float32Array[]> {\n    if (!tokens.length) {\n      return []\n    }\n\n    const vectors: Float32Array[] = Array(tokens.length)\n    const idxToFetch: number[] = [] // the tokens we need to fetch remotely\n    const getCacheKey = (t: string) => `${lang}_${encodeURI(t)}`\n\n    tokens.forEach((token, i) => {\n      if (isSpace(token)) {\n        vectors[i] = new Float32Array(this._languageDims) // float 32 Arrays are initialized with 0s\n      } else if (this._vectorsCache.has(getCacheKey(token))) {\n        vectors[i] = this._vectorsCache.get(getCacheKey(token))\n      } else {\n        idxToFetch.push(i)\n      }\n    })\n\n    while (idxToFetch.length) {\n      // we tokenize maximum 100 tokens at the same time\n      const group = idxToFetch.splice(0, 100)\n\n      // We have new tokens we haven't cached yet\n      const query = group.map(idx => tokens[idx].toLowerCase())\n      // Fetch only the missing tokens\n      if (!query.length) {\n        break\n      }\n\n      const fetched = await this.queryProvider<number[][]>(lang, '/vectorize', { tokens: query }, 'vectors')\n\n      if (fetched.length !== query.length) {\n        throw new Error(\n          `Language Provider didn't receive as many vectors as we asked for (asked ${query.length} and received ${fetched.length})`\n        )\n      }\n\n      // Reconstruct them in our array and cache them for future cache lookup\n      group.forEach((tokenIdx, fetchIdx) => {\n        vectors[tokenIdx] = Float32Array.from(fetched[fetchIdx])\n        this._vectorsCache.set(getCacheKey(tokens[tokenIdx]), vectors[tokenIdx])\n      })\n\n      await this.onVectorsCacheChanged()\n    }\n\n    return vectors\n  }\n\n  async tokenize(utterances: string[], lang: string, vocab: Token2Vec = {}): Promise<string[][]> {\n    if (!utterances.length) {\n      return []\n    }\n\n    const getCacheKey = (t: string) => `${lang}_${encodeURI(t)}`\n    const tokenUtterances: string[][] = Array(utterances.length)\n    const idxToFetch: number[] = [] // the utterances we need to fetch remotely\n\n    utterances.forEach((utterance, idx) => {\n      if (this._tokensCache.has(getCacheKey(utterance))) {\n        tokenUtterances[idx] = this._tokensCache.get(getCacheKey(utterance))\n      } else {\n        idxToFetch.push(idx)\n      }\n    })\n\n    // At this point, final[] contains the utterances we had cached\n    // It has some \"holes\", we kept track of the indices where those wholes are in `idxToFetch`\n\n    while (idxToFetch.length) {\n      // While there's utterances we haven't tokenized yet\n      // We're going to batch requests by maximum 150KB worth's of utterances\n      let totalSize = 0\n      const sliceUntil = idxToFetch.reduce((topIdx, idx, i) => {\n        if ((totalSize += utterances[idx].length * 4) < MAX_PAYLOAD_SIZE) {\n          return i\n        } else {\n          return topIdx\n        }\n      }, 0)\n      const batch = idxToFetch.splice(0, sliceUntil + 1)\n      const query = batch.map(idx => utterances[idx].toLowerCase())\n\n      if (!query.length) {\n        break\n      }\n\n      let fetched = await this.queryProvider<string[][]>(lang, '/tokenize', { utterances: query }, 'tokens')\n      fetched = fetched.map(toks => processUtteranceTokens(toks, vocab))\n\n      if (fetched.length !== query.length) {\n        throw new Error(\n          `Language Provider didn't receive as many utterances as we asked for (asked ${query.length} and received ${fetched.length})`\n        )\n      }\n\n      // Reconstruct them in our array and cache them for future cache lookup\n      batch.forEach((utteranceIdx, fetchIdx) => {\n        tokenUtterances[utteranceIdx] = Array.from(fetched[fetchIdx])\n        this._tokensCache.set(getCacheKey(utterances[utteranceIdx]), tokenUtterances[utteranceIdx])\n      })\n\n      await this.onTokensCacheChanged()\n    }\n\n    // we restore original chars and casing\n    return tokenUtterances.map((tokens, i) => restoreOriginalUtteranceCasing(tokens, utterances[i]))\n  }\n}\n\nexport default new RemoteLanguageProvider()\n"]}